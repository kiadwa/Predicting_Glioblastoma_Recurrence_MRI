{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d94cb2",
   "metadata": {},
   "source": [
    "\n",
    "# Glioblastoma Recurrence Prediction â€“ Modern Notebook Pipeline\n",
    "\n",
    "This notebook reproduces the voxel-wise recurrence prediction workflow from the repository while relying on a modern Python stack. It guides you through environment setup, data preparation, inference, and visualisation so that the project can be executed inside an interactive environment such as JupyterLab or Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677b09f",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "The original scripts were pinned to an older set of dependencies. The cell below installs an updated yet compatible stack that has been verified to work with the current code base. Feel free to adapt the versions if your infrastructure requires it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dead3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q     matplotlib==3.8.4     nibabel==5.2.1     numpy==1.26.4     pandas==2.2.2     pyradiomics==3.1.0     scikit-image==0.23.2     scikit-learn==1.4.2     scipy==1.11.4     SimpleITK==2.3.1     tqdm==4.66.4     PyWavelets==1.6.0     pyarrow==16.1.0     fastparquet==2024.5.0     trimesh==4.4.4     xgboost==2.0.3     lightgbm==4.3.0     catboost==1.2.5     pydensecrf==1.0rc3     wandb==0.17.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e3092",
   "metadata": {},
   "source": [
    "\n",
    "> ðŸ’¡ **Tip:** Restart the kernel after the installation finishes to ensure the updated libraries are picked up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92688cc9",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports and configuration\n",
    "\n",
    "This section loads the helper utilities that already ship with the repository and defines a few convenience wrappers that are more notebook-friendly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "from extract_feature_functions import create_dataset, retrieve_patient_data\n",
    "from utils import correct_proba, fuse_t1ce_and_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_ROOT = Path(\"Patients\")  # folder that contains one sub-directory per patient\n",
    "MODEL_PATH = Path(\"model.pkl\")  # pre-trained ensemble supplied with the project\n",
    "MAXIMUM_DISTANCE_MM = 20  # attenuation radius used during post-processing\n",
    "USE_DISTANCE_CORRECTION = True  # toggle if you want to skip distance attenuation\n",
    "\n",
    "assert DATA_ROOT.exists(), f\"Patient directory not found: {DATA_ROOT.resolve()}\"\n",
    "assert MODEL_PATH.exists(), (\n",
    "    \"The pre-trained model is missing. Download `model.pkl` from the official \"\n",
    "    \"release package and place it in the repository root.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c53e0",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Feature extraction (voxel-wise radiomics)\n",
    "\n",
    "The helper `create_dataset` function checks every patient directory for a cached `voxel_features.parquet` file. If it is missing, radiomic features will be extracted with PyRadiomics using the parameters provided in `Params.yaml`.\n",
    "\n",
    "The cell below may take a while the first time because it processes each MRI sequence independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be27787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patient_dirs = create_dataset(str(DATA_ROOT))\n",
    "print(f\"Discovered {len(patient_dirs)} patients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfa499",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Inference â€“ predicting voxel-level recurrence risk\n",
    "\n",
    "We now replicate the core logic of `main.py` in a notebook-friendly function. It loads the scaler and CatBoost classifier from `model.pkl`, runs inference for each patient, optionally applies the distance-based attenuation heuristic, and stores the resulting voxel probabilities and binary predictions as parquet files. Finally, it generates the fused DICOM visualisation for convenient review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c42f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inference(\n",
    "    patient_paths: list[str],\n",
    "    model_path: Path,\n",
    "    maximum_distance: float | int = 20,\n",
    "    apply_distance_correction: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Execute the trained classifier on each patient directory.\"\"\"\n",
    "\n",
    "    with model_path.open(\"rb\") as f:\n",
    "        models_dict, scaler, metrics_dict, metadata = pickle.load(f)\n",
    "\n",
    "    if \"CAT\" not in models_dict:\n",
    "        raise KeyError(\"The loaded model archive does not contain a CatBoost classifier under the 'CAT' key.\")\n",
    "\n",
    "    model = models_dict[\"CAT\"]\n",
    "\n",
    "    for patient in tqdm(patient_paths, desc=\"Patients\"):\n",
    "        patient_dir = Path(patient)\n",
    "        X = retrieve_patient_data(str(patient_dir), scaler)\n",
    "\n",
    "        voxel_probabilities = model.predict_proba(X)[:, 1]\n",
    "        if apply_distance_correction and maximum_distance:\n",
    "            voxel_probabilities = correct_proba(str(patient_dir), voxel_probabilities, maximum_distance)\n",
    "\n",
    "        threshold = threshold_otsu(voxel_probabilities)\n",
    "        voxel_predictions = voxel_probabilities > threshold\n",
    "\n",
    "        output = pd.DataFrame(\n",
    "            {\"predictions\": voxel_predictions, \"probabilities\": voxel_probabilities},\n",
    "            index=X.index,\n",
    "        )\n",
    "        output_path = patient_dir / \"predictions.parquet\"\n",
    "        output.to_parquet(output_path)\n",
    "\n",
    "        fuse_t1ce_and_proba(str(patient_dir))\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"Saved probabilities to {output_path} and generated fused visualisations in \"\n",
    "            f\"{patient_dir / 'saved_images'}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_inference(\n",
    "    patient_dirs,\n",
    "    MODEL_PATH,\n",
    "    maximum_distance=MAXIMUM_DISTANCE_MM,\n",
    "    apply_distance_correction=USE_DISTANCE_CORRECTION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d7c39",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Inspecting the outputs\n",
    "\n",
    "Each patient folder now contains:\n",
    "\n",
    "- `voxel_features.parquet`: cached radiomic features for every voxel inside the peritumoural mask.\n",
    "- `predictions.parquet`: voxel-wise recurrence probabilities and binary labels.\n",
    "- `saved_images/probabilities.nii`: 3D NIfTI volume of the probability map.\n",
    "- `saved_images/t1ce_fused_proba.dcm`: colour overlay that can be reviewed in any DICOM viewer.\n",
    "\n",
    "The snippet below illustrates how to load and inspect the parquet data directly from the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03721fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_patient = Path(patient_dirs[0])\n",
    "probabilities_df = pd.read_parquet(example_patient / \"predictions.parquet\")\n",
    "probabilities_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1391e3",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Optional: Visualising the probability map inline\n",
    "\n",
    "You can leverage `nibabel` and `matplotlib` to render slices from the probability heatmap within the notebook. This is particularly useful when working inside Colab or JupyterLab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "prob_volume = nib.load(example_patient / \"saved_images\" / \"probabilities.nii\").get_fdata()\n",
    "\n",
    "slice_index = np.nanargmax(np.nanmean(prob_volume, axis=(0, 1)))\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(prob_volume[:, :, slice_index].T, cmap=\"turbo\", origin=\"lower\")\n",
    "plt.title(f\"Probability map â€“ axial slice {slice_index}\")\n",
    "plt.colorbar(label=\"Recurrence probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34a04a",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Next steps\n",
    "\n",
    "- Integrate the notebook into your clinical research workflow by adapting the pre-processing stage or exporting the predictions to other formats.\n",
    "- If you want to retrain the model, inspect `sweep.yaml` and the training utilities in the repository as a starting point.\n",
    "- Consider wrapping the notebook into a reproducible Docker/Colab environment for easier sharing.\n",
    "\n",
    "Happy experimenting! ðŸ§ \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
