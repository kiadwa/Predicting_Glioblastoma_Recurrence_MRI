{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e147cd",
   "metadata": {},
   "source": [
    "\n",
    "# Train a CatBoost Voxel-wise Recurrence Classifier\n",
    "\n",
    "This notebook provides a scaffolding workflow to recreate the voxel-level glioblastoma recurrence model used by the inference pipeline. It mirrors the repository's feature extraction, scaling, and CatBoost training steps while leaving space for you to plug in your own cohort specifics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4617c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "Install a recent toolchain that aligns with CatBoost and PyRadiomics. The versions below have been validated together, but you can adjust them as long as the APIs remain compatible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install --quiet --upgrade     catboost==1.2.5     pandas>=2.1     pyarrow>=12.0     pyradiomics==3.1.0     scikit-learn>=1.3     SimpleITK>=2.2     nibabel>=5.1     tqdm>=4.66\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d0671",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports & paths\n",
    "\n",
    "Point the configuration at your pre-processed dataset. Each patient folder should contain the five MRI sequences, the peritumoral mask, and a recurrence (or recurrence-free) label map for supervision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd668a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Repository utilities (assumes this notebook lives inside the repo)\n",
    "from extract_feature_functions import create_dataset\n",
    "\n",
    "# Configure the root directory that contains individual patient sub-folders\n",
    "DATA_ROOT = Path(\"Patients\")  # TODO: update with your actual dataset location\n",
    "\n",
    "# Name of the segmentation containing voxel-wise supervision labels\n",
    "RECURRENCE_MASK_NAME = \"recurrence.nii.gz\"  # TODO: adjust to your label file name\n",
    "\n",
    "# Destination for the fitted scaler and CatBoost model\n",
    "OUTPUT_DIR = Path(\"artifacts\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea698504",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Feature extraction\n",
    "\n",
    "Run the repository's voxel-wise extractor to ensure every patient has a `voxel_features.parquet`. If you already generated them with the inference pipeline you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This call mirrors the production pipeline and uses Params.yaml automatically.\n",
    "# It only processes patients that are missing their parquet feature tables.\n",
    "create_dataset(str(DATA_ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37850128",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Assemble voxel-level design matrix\n",
    "\n",
    "The helper below loads each patient's radiomic features, imputes NaNs with per-feature means, and applies a scaler once it is fitted. During the initial pass we collect the raw features to derive the scaler parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65205a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nibabel as nib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Collect per-voxel features and labels for every patient\n",
    "feature_frames = []\n",
    "label_frames = []\n",
    "patient_ids = []\n",
    "\n",
    "for patient_dir in tqdm(sorted(DATA_ROOT.iterdir()), desc=\"Patients\"):\n",
    "    if not patient_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    voxel_features_path = patient_dir / \"voxel_features.parquet\"\n",
    "    recurrence_mask_path = patient_dir / RECURRENCE_MASK_NAME\n",
    "\n",
    "    if not voxel_features_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing features for {patient_dir.name}; run the extractor first\")\n",
    "    if not recurrence_mask_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing recurrence labels ({RECURRENCE_MASK_NAME}) for {patient_dir.name}\")\n",
    "\n",
    "    # Load features as produced by the repo utilities\n",
    "    features = pd.read_parquet(voxel_features_path)\n",
    "\n",
    "    # Derive the voxel-level binary label from the recurrence segmentation\n",
    "    recurrence_mask = nib.load(str(recurrence_mask_path)).get_fdata().astype(bool).ravel()\n",
    "\n",
    "    # Restrict labels to voxels that belong to the peritumoral ROI (same ordering as features)\n",
    "    if features.shape[0] != recurrence_mask.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Label mask for {patient_dir.name} does not match feature voxel count: \"\n",
    "            f\"{recurrence_mask.shape[0]} vs {features.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    feature_frames.append(features)\n",
    "    label_frames.append(pd.Series(recurrence_mask.astype(np.uint8), index=features.index))\n",
    "    patient_ids.extend([patient_dir.name] * len(features))\n",
    "\n",
    "X_raw = pd.concat(feature_frames, axis=0).reset_index(drop=True)\n",
    "y = pd.concat(label_frames, axis=0).reset_index(drop=True)\n",
    "patient_ids = np.array(patient_ids)\n",
    "\n",
    "print(f\"Aggregated {len(X_raw)} voxels across {len(np.unique(patient_ids))} patients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27263185",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Handle missing values & scale features\n",
    "\n",
    "The training notebook replicates the inference-time preprocessing by imputing each feature with its mean and fitting a `StandardScaler` (or whichever transformer you prefer). Save the fitted scaler to reuse during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Impute NaN values with column means\n",
    "X_imputed = X_raw.fillna(X_raw.mean())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Persist the fitted scaler for later use\n",
    "scaler_path = OUTPUT_DIR / \"voxel_scaler.joblib\"\n",
    "import joblib\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afed83",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Train/validation split\n",
    "\n",
    "Construct a patient-level split to avoid voxel leakage. Adjust the strategy if you need cross-validation or stratification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Derive patient-level indices for splitting\n",
    "unique_patients = np.unique(patient_ids)\n",
    "train_patients, valid_patients = train_test_split(\n",
    "    unique_patients,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_mask = np.isin(patient_ids, train_patients)\n",
    "valid_mask = np.isin(patient_ids, valid_patients)\n",
    "\n",
    "X_train, X_valid = X_scaled[train_mask], X_scaled[valid_mask]\n",
    "y_train, y_valid = y.iloc[train_mask], y.iloc[valid_mask]\n",
    "\n",
    "print(f\"Training voxels: {X_train.shape[0]} | Validation voxels: {X_valid.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6c3fa",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Configure and train CatBoost\n",
    "\n",
    "Define the CatBoost hyperparameters that mirror the production model. The block below uses binary logloss with class weights to mitigate imbalance, but you can tune as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ee74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_params = dict(\n",
    "    loss_function=\"Logloss\",\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    iterations=2000,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    class_weights=None,  # e.g., {0: 1.0, 1: 5.0} if recurrence voxels are rare\n",
    ")\n",
    "\n",
    "train_pool = Pool(X_train, label=y_train)\n",
    "valid_pool = Pool(X_valid, label=y_valid)\n",
    "\n",
    "model = CatBoostClassifier(**cat_params)\n",
    "model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001b1bc",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Evaluate & inspect\n",
    "\n",
    "Use CatBoost's built-in metrics or scikit-learn utilities to quantify performance. The following skeleton computes AUC as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "valid_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "auc = roc_auc_score(y_valid, valid_pred_proba)\n",
    "print(f\"Validation ROC-AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ee379",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Persist the training artefacts\n",
    "\n",
    "Export the CatBoost model together with the feature names and scaler metadata expected by the inference pipeline. The pickle format mirrors the structure that `main.py` consumes (`{\"scaler\": ..., \"models_dict\": {\"CAT\": model}}`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20add9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = OUTPUT_DIR / \"catboost_voxel_model.cbm\"\n",
    "model.save_model(model_path)\n",
    "print(f\"CatBoost model saved to {model_path}\")\n",
    "\n",
    "# Bundle scaler + model into the repository's expected pickle structure\n",
    "import pickle\n",
    "\n",
    "pickle_payload = {\n",
    "    \"scaler\": scaler,\n",
    "    \"models_dict\": {\"CAT\": model},\n",
    "    \"feature_names\": X_raw.columns.tolist(),\n",
    "    \"catboost_params\": cat_params,\n",
    "}\n",
    "\n",
    "pickle_path = OUTPUT_DIR / \"model.pkl\"\n",
    "with open(pickle_path, \"wb\") as f:\n",
    "    pickle.dump(pickle_payload, f)\n",
    "\n",
    "print(f\"Serialized inference bundle saved to {pickle_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747da96",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Next steps\n",
    "\n",
    "- Perform hyperparameter tuning (e.g., cross-validation, Bayesian optimisation) for better performance.\n",
    "- Incorporate class balancing strategies if recurrence voxels are scarce.\n",
    "- Track experiments with tools such as Weights & Biases or MLflow.\n",
    "- Validate predictions on held-out patients before deploying the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
