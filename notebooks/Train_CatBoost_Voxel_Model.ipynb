{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e147cd",
   "metadata": {},
   "source": [
    "\n",
    "# Train a CatBoost Voxel-wise Recurrence Classifier\n",
    "\n",
    "This notebook provides a scaffolding workflow to recreate the voxel-level glioblastoma recurrence model used by the inference pipeline. It mirrors the repository's feature extraction, scaling, and CatBoost training steps while leaving space for you to plug in your own cohort specifics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4617c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "Install a recent toolchain that aligns with CatBoost and PyRadiomics. The versions below have been validated together, but you can adjust them as long as the APIs remain compatible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d54459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No suitable Python runtime found\n",
      "Pass --list (-0) to see all detected environments on your machine\n",
      "or set environment variable PYLAUNCHER_ALLOW_INSTALL to use winget\n",
      "or open the Microsoft Store to the requested version.\n",
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\phkya\\documents\\documents\\thesis2025\\predicting_glioblastoma_recurrence_mri\\venv\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/b7/3f/945ef7ab14dc4f9d7f40288d2df998d1837ee0888ec3659c813487572faa/pip-25.2-py3-none-any.whl.metadata\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting setuptools\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/a3/dc/17031897dae0efacfea57dfd3a82fdd2a2aeb58e0ff71b77b87e44edc772/setuptools-80.9.0-py3-none-any.whl.metadata\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting wheel\n",
      "  Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl.metadata\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 660.6 kB/s eta 0:00:03\n",
      "    --------------------------------------- 0.0/1.8 MB 487.6 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/1.8 MB 653.6 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/1.8 MB 798.9 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/1.8 MB 1.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/1.8 MB 1.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/1.8 MB 1.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.8 MB 1.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/1.8 MB 995.6 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.6/1.8 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.7/1.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.7/1.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.1/1.8 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.2/1.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.7/1.8 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 1.5 MB/s eta 0:00:00\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-25.2 setuptools-80.9.0 wheel-0.45.1\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pandas==2.1.4\n",
      "  Using cached pandas-2.1.4-cp312-cp312-win_amd64.whl.metadata (18 kB)\n",
      "Collecting pyarrow==14.0.2\n",
      "  Using cached pyarrow-14.0.2-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting scikit-learn==1.3.2\n",
      "  Using cached scikit_learn-1.3.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement SimpleITK==2.2.1 (from versions: 1.0.1, 1.2.0, 2.1.0, 2.1.1.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.2)\n",
      "ERROR: No matching distribution found for SimpleITK==2.2.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec gbm-ml in C:\\Users\\phkya\\AppData\\Roaming\\jupyter\\kernels\\gbm-ml\n"
     ]
    }
   ],
   "source": [
    "# Install Python 3.10 first if you donâ€™t have it\n",
    "!py -3.10 -m venv .venv\n",
    "!.\\.venv\\Scripts\\activate\n",
    "!python -m pip install --upgrade pip setuptools wheel\n",
    "%pip install numpy==1.26.4 pandas==2.1.4 pyarrow==14.0.2  scikit-learn==1.3.2 SimpleITK==2.2.1 nibabel==5.1.0 tqdm==4.66.1 catboost==1.2.5 pyradiomics==3.0.1\n",
    "!python -m ipykernel install --user --name gbm-ml --display-name \"Python 3.10 (gbm-ml)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d0671",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports & paths\n",
    "\n",
    "Point the configuration at your pre-processed dataset. Each patient folder should contain the five MRI sequences, the peritumoral mask, and a recurrence (or recurrence-free) label map for supervision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd668a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Repository utilities (assumes this notebook lives inside the repo)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# from .extract_feature_functions import create_dataset\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextract_feature_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_dataset\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Configure the root directory that contains individual patient sub-folders\u001b[39;00m\n\u001b[0;32m     15\u001b[0m DATA_ROOT \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatients\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# TODO: update with your actual dataset location\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from ..extract_feature_functions import create_dataset\n",
    "\n",
    "# Configure the root directory that contains individual patient sub-folders\n",
    "DATA_ROOT = Path(\"Patients\")  # TODO: update with your actual dataset location\n",
    "\n",
    "# Name of the segmentation containing voxel-wise supervision labels\n",
    "RECURRENCE_MASK_NAME = \"recurrence.nii.gz\"  # TODO: adjust to your label file name\n",
    "\n",
    "# Destination for the fitted scaler and CatBoost model\n",
    "OUTPUT_DIR = Path(\"artifacts\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea698504",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Feature extraction\n",
    "\n",
    "Run the repository's voxel-wise extractor to ensure every patient has a `voxel_features.parquet`. If you already generated them with the inference pipeline you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5a8028",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This call mirrors the production pipeline and uses Params.yaml automatically.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# It only processes patients that are missing their parquet feature tables.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mcreate_dataset\u001b[49m(\u001b[38;5;28mstr\u001b[39m(DATA_ROOT))\n",
      "\u001b[31mNameError\u001b[39m: name 'create_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# This call mirrors the production pipeline and uses Params.yaml automatically.\n",
    "# It only processes patients that are missing their parquet feature tables.\n",
    "create_dataset(str(DATA_ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37850128",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Assemble voxel-level design matrix\n",
    "\n",
    "The helper below loads each patient's radiomic features, imputes NaNs with per-feature means, and applies a scaler once it is fitted. During the initial pass we collect the raw features to derive the scaler parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65205a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nibabel as nib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Collect per-voxel features and labels for every patient\n",
    "feature_frames = []\n",
    "label_frames = []\n",
    "patient_ids = []\n",
    "\n",
    "for patient_dir in tqdm(sorted(DATA_ROOT.iterdir()), desc=\"Patients\"):\n",
    "    if not patient_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    voxel_features_path = patient_dir / \"voxel_features.parquet\"\n",
    "    recurrence_mask_path = patient_dir / RECURRENCE_MASK_NAME\n",
    "\n",
    "    if not voxel_features_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing features for {patient_dir.name}; run the extractor first\")\n",
    "    if not recurrence_mask_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing recurrence labels ({RECURRENCE_MASK_NAME}) for {patient_dir.name}\")\n",
    "\n",
    "    # Load features as produced by the repo utilities\n",
    "    features = pd.read_parquet(voxel_features_path)\n",
    "\n",
    "    # Derive the voxel-level binary label from the recurrence segmentation\n",
    "    recurrence_mask = nib.load(str(recurrence_mask_path)).get_fdata().astype(bool).ravel()\n",
    "\n",
    "    # Restrict labels to voxels that belong to the peritumoral ROI (same ordering as features)\n",
    "    if features.shape[0] != recurrence_mask.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Label mask for {patient_dir.name} does not match feature voxel count: \"\n",
    "            f\"{recurrence_mask.shape[0]} vs {features.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    feature_frames.append(features)\n",
    "    label_frames.append(pd.Series(recurrence_mask.astype(np.uint8), index=features.index))\n",
    "    patient_ids.extend([patient_dir.name] * len(features))\n",
    "\n",
    "X_raw = pd.concat(feature_frames, axis=0).reset_index(drop=True)\n",
    "y = pd.concat(label_frames, axis=0).reset_index(drop=True)\n",
    "patient_ids = np.array(patient_ids)\n",
    "\n",
    "print(f\"Aggregated {len(X_raw)} voxels across {len(np.unique(patient_ids))} patients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27263185",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Handle missing values & scale features\n",
    "\n",
    "The training notebook replicates the inference-time preprocessing by imputing each feature with its mean and fitting a `StandardScaler` (or whichever transformer you prefer). Save the fitted scaler to reuse during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Impute NaN values with column means\n",
    "X_imputed = X_raw.fillna(X_raw.mean())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Persist the fitted scaler for later use\n",
    "scaler_path = OUTPUT_DIR / \"voxel_scaler.joblib\"\n",
    "import joblib\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afed83",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Train/validation split\n",
    "\n",
    "Construct a patient-level split to avoid voxel leakage. Adjust the strategy if you need cross-validation or stratification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Derive patient-level indices for splitting\n",
    "unique_patients = np.unique(patient_ids)\n",
    "train_patients, valid_patients = train_test_split(\n",
    "    unique_patients,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_mask = np.isin(patient_ids, train_patients)\n",
    "valid_mask = np.isin(patient_ids, valid_patients)\n",
    "\n",
    "X_train, X_valid = X_scaled[train_mask], X_scaled[valid_mask]\n",
    "y_train, y_valid = y.iloc[train_mask], y.iloc[valid_mask]\n",
    "\n",
    "print(f\"Training voxels: {X_train.shape[0]} | Validation voxels: {X_valid.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6c3fa",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Configure and train CatBoost\n",
    "\n",
    "Define the CatBoost hyperparameters that mirror the production model. The block below uses binary logloss with class weights to mitigate imbalance, but you can tune as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ee74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_params = dict(\n",
    "    loss_function=\"Logloss\",\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    iterations=2000,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    class_weights=None,  # e.g., {0: 1.0, 1: 5.0} if recurrence voxels are rare\n",
    ")\n",
    "\n",
    "train_pool = Pool(X_train, label=y_train)\n",
    "valid_pool = Pool(X_valid, label=y_valid)\n",
    "\n",
    "model = CatBoostClassifier(**cat_params)\n",
    "model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001b1bc",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Evaluate & inspect\n",
    "\n",
    "Use CatBoost's built-in metrics or scikit-learn utilities to quantify performance. The following skeleton computes AUC as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "valid_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "auc = roc_auc_score(y_valid, valid_pred_proba)\n",
    "print(f\"Validation ROC-AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ee379",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Persist the training artefacts\n",
    "\n",
    "Export the CatBoost model together with the feature names and scaler metadata expected by the inference pipeline. The pickle format mirrors the structure that `main.py` consumes (`{\"scaler\": ..., \"models_dict\": {\"CAT\": model}}`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20add9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = OUTPUT_DIR / \"catboost_voxel_model.cbm\"\n",
    "model.save_model(model_path)\n",
    "print(f\"CatBoost model saved to {model_path}\")\n",
    "\n",
    "# Bundle scaler + model into the repository's expected pickle structure\n",
    "import pickle\n",
    "\n",
    "pickle_payload = {\n",
    "    \"scaler\": scaler,\n",
    "    \"models_dict\": {\"CAT\": model},\n",
    "    \"feature_names\": X_raw.columns.tolist(),\n",
    "    \"catboost_params\": cat_params,\n",
    "}\n",
    "\n",
    "pickle_path = OUTPUT_DIR / \"model.pkl\"\n",
    "with open(pickle_path, \"wb\") as f:\n",
    "    pickle.dump(pickle_payload, f)\n",
    "\n",
    "print(f\"Serialized inference bundle saved to {pickle_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747da96",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Next steps\n",
    "\n",
    "- Perform hyperparameter tuning (e.g., cross-validation, Bayesian optimisation) for better performance.\n",
    "- Incorporate class balancing strategies if recurrence voxels are scarce.\n",
    "- Track experiments with tools such as Weights & Biases or MLflow.\n",
    "- Validate predictions on held-out patients before deploying the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
